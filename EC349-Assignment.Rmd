---
title: "EC349-Assignment"
author: "Travis Tan"
date: "2023-12-04"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Tabula Statement

We're part of an academic community at Warwick.

Whether studying, teaching, or researching, we’re all taking part in an expert conversation which must meet standards of academic integrity. When we all meet these standards, we can take pride in our own academic achievements, as individuals and as an academic community.

Academic integrity means committing to honesty in academic work, giving credit where we've used others' ideas and being proud of our own achievements.

In submitting my work I confirm that:

1. I have read the guidance on academic integrity provided in the Student Handbook and understand the University regulations in relation to Academic Integrity. I am aware of the potential consequences of Academic Misconduct.

2. I declare that the work is all my own, except where I have stated otherwise.

3. No substantial part(s) of the work submitted here has also been submitted by me in other credit bearing assessments courses of study (other than in certain cases of a resubmission of a piece of work), and I acknowledge that if this has been done this may lead to an appropriate sanction.

4. Where a generative Artificial Intelligence such as ChatGPT has been used I confirm I have abided by both the University guidance and specific requirements as set out in the Student Handbook and the Assessment brief. I have clearly acknowledged the use of any generative Artificial Intelligence in my submission, my reasoning for using it and which generative AI (or AIs) I have used. Except where indicated the work is otherwise entirely my own.

5. I understand that should this piece of work raise concerns requiring investigation in relation to any of points above, it is possible that other work I have submitted for assessment will be checked, even if marks (provisional or confirmed) have been published.

6. Where a proof-reader, paid or unpaid was used, I confirm that the proofreader was made aware of and has complied with the University’s proofreading policy.

7. I consent that my work may be submitted to Turnitin or other analytical technology. I understand the use of this service (or similar), along with other methods of maintaining the integrity of the academic process, will help the University uphold academic standards and assessment fairness.

## Privacy Statement

The data on this form relates to your submission of coursework. The date and time of your submission, your identity, and the work you have submitted will be stored. We will only use this data to administer and record your coursework submission.

## Related Articles

- [Reg. 11 Academic Integrity (from 4 Oct 2021)](https://warwick.ac.uk/services/aro/dar/quality/categories/examinations/regulations/cheating)
- [Guidance on Regulation 11](https://warwick.ac.uk/services/aro/dar/quality/categories/examinations/regulations/cheating/guidance)
- [Proofreading Policy](https://warwick.ac.uk/services/aro/dar/quality/categories/proofreading)
- [Education Policy and Quality Team](https://warwick.ac.uk/services/aro/dar/quality)
- [Academic Integrity (warwick.ac.uk)](https://warwick.ac.uk/services/aro/dar/quality/categories/examinations/academicintegrity)

This is the end of the statement to be included.  


### Introduction
For this project, I chose John Rollins’ methodology for its flexible, iterative nature due to challenges in data wrangling, comprehensiveness in analysis and agility in implementation, making it suitable for a big data analysis such as this. This methodology emphasizes an iterative process through phases including data understanding, preparation, modelling, evaluation, and deployment. I began with data understanding as the previous stages have already been carried out and iterated continuously between data preparation and understanding until I was satisfied with my data before modelling. However, I encountered several challenges while modelling such as running out of RAM to compute my desired models, possible collinear variables requiring feature refinement and this methodology allowed me to iterate between phases multiple times till my desired model was achieved.  

### Most difficult challenge in assignment
The most challenging aspect of this project was deciding how to wrangle the business dataset, engineer features, and extract the most relevant information in predicting stars with relatively simple models as taught in class. The business dataset has 39 mostly unfilled attributes and the many attributes that were hard to transform meaningfully for fitting models such as ambience and the unreliable information made usefulness in analysing individual attributes limited.  

To solve this issue, the total number of non-empty attributes for every business was summed up and named the variable “engagement”. The more attributes a business fills out, even if the response is none, the more likely a business is to be engaged on Yelp and other platforms, the more likely their proactiveness to maintain a digital presence and outreach. These businesses are also more likely to strive for better reviews on Yelp and could be another feature for predicting stars. This was a feasible way I found to extract information out of attributes I wouldn’t otherwise be able to.

### Data Understanding and Variable Selection
I began data wrangling with user data to extract useful user features which may aid in review prediction. I started with finding user account age with by using 2023/01/01 as the base date for age. I then calculated “friend_count” and “elite_year_count” by parsing the columns. I originally summed up the total number of compliments for each user but decided the number of compliment variables wouldn’t overfit the model and kept the original. I included other user features like average stars, total fans, total useful, funny, and cool sent by users. I believe these features would show how active a user might be and would help in review prediction. I also included the user tip count extracted from tip data as another feature for a user’s proactiveness on yelp.  

In addition to the aforementioned challenges in business data wrangling, I found 1,312 unique business categories after transforming the categories into a table of unique categories, simply too much for any analysis. I originally parsed the categories for main categories found on Yelp’s site but refined it further by identifying important categories by considering engagement, as a more engaged business’s category would likely improve prediction accuracy. I plotted the engagement means of each major category and included the categories with the highest engagement means as dummies for the model with reasoning as above. Bars, restaurants, and grocery appear to be the strongest categories as I examined the data, so I included them in the plot, which suggests my suspicions were correct. Transforming business hours for use also proved challenging as I found 23,223 businesses with no information while some businesses only have hours listed for less than 2 days a week, often a temporary measure, after checking on Yelp. I decided to omit “is open” and opening hours as I believe they are both weak and noisy predictors while the data is unreliable for hours. I also collected the check-in and tip counts a business received as another feature of how popular the business may be. I included latitude, longitude, and state data as geographical features, but omitted cities for two reasons, as there would be over 1000 levels which R cannot handle and would likely also overfit the model.  
  
![](Engagement by category.png)  
  

Finally, I included review age, and ratings a review received for my model. While I had originally used the original review dataset for this project, I used the smaller dataset of 1.4 million observations instead as I could not train a Random Forests model otherwise.

  
  
### Models’ Training and Analysis:
As review stars are discrete outcomes, I believe a tree model would be suitable for this classification. However, individual decision trees are not robust enough as a small change in data could significantly change predictions. Bagging aims to alleviate this by growing many trees while randomly sampling the training data with replacement and taking the averages of trees’ predictions, however there is still correlation between trees predictions so there is a limit in gains in robustness through bagging. Random forest address this through also sub-setting variables randomly for every split in every tree on top of bagging, which should reduce the correlation in predictions across the tree ensemble. Overall, I believe Random Forest would be most robust and suited for this project, even with interpretability trade-offs compared to linear models. Nonetheless, I started with fitting other models for more information on my selected features. I found that my LASSO model performed variable selection for only one variable both before and after scaling, unexpectedly my scaled LASSO model still had an intercept. 
```{r}
library(knitr)
load("C:/Users/Travis Tan/OneDrive - University of Warwick/EC349/R projects/EC349-Assignment/LASSO coefficients.Rdata")
kable(coef.lasso.df)
```

```{r}
library(knitr)
load("C:/Users/Travis Tan/OneDrive - University of Warwick/EC349/R projects/EC349-Assignment/scaled LASSO coefficients.Rdata")
kable(coef.scaled.lasso.df)
```

This may imply that most variables included were relatively similar in explanatory power which suggests Random Forest could be a suitable approach as the main weakness of Random Forests is poor performance when variable size is large but only a small number of variables are relevant. This may also imply that random forests would likely outperform bagging with this main weakness dispelled since Random Forest usually outperform bagging through eliminating correlation between trees by sub-setting variables when growing trees. If only few variables are relevant, Random Forest is likely to subset irrelevant features when growing trees whereas in bagging, there's a consistent chance that these relevant features would be used to grow trees since bagging does not subset features.  

While a boosting model such as AdaBoost was an option, I chose Random Forest for its robustness to overfitting given my large feature set as Random Forest’s averaging is less sensitive to noise in the data compared to boosting which continuously corrects previous mistakes and so is less sensitive to outliers. Random Forest is also simpler to tune compared to boosting as boosting would require tuning the weights of predictors averaged and the number of estimators predicted.  

After estimating the models, Random Forest was confirmed to be the best performing model even compared to boosting, with a 200-tree-forest generating a mean test error rate of 0.4007445, meaning Random Forest outputs correct predictions 60% of the time. This is compared to error rates of 0.4803091 for bagging and 0.4529025 for boosting, validating my suspicions.  

### Conclusion

Although Random Forest outperformed boosting in my analysis, boosting or a neural network could outperform Random Forest through careful tuning. Although Random Forest’s predictions were the most accurate of classification models in my tests, a 0.4 mean error rate definitely leaves room for improvement. While using a bigger dataset may bring prediction improvements, I believe the dataset is sufficiently big and any accuracy gains would be minimal.  

  
Link to GitHub project : https://github.com/TTravis67/EC349-Assignment




